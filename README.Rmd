---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```
# repvar

[![lifecycle](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![Travis build status](https://travis-ci.org/zkamvar/repvar.svg?branch=master)](https://travis-ci.org/zkamvar/repvar)
[![Coverage status](https://codecov.io/gh/zkamvar/repvar/branch/master/graph/badge.svg)](https://codecov.io/github/zkamvar/repvar?branch=master)

The goal of repvar is to find the minimum number of samples that will represent
all variables in a data set. This was built for population genetic data, but is
generalizable to any discrete data type that can be represented as an integer
matrix. 

## Installation

This package is not currently on CRAN, but you can install it like so:

```{r inst, eval = FALSE}
# install.packages("remotes") # or devtools or ghit
remotes::install_github("zkamvar/repvar")
```


## Example

Here is a basic example of how you can identify the minimum set. We will use the
pre-packaged `monilinia` data set from Everhart and Scherm 2016.

```{r example}
options(width = 120)

library("repvar")
data(monilinia)
dim(monilinia)
print.table(monilinia[1:10, 1:10], zero.print = ".")

# Shuffle the data set 200 times to find an optimal number of samples
set.seed(2018)
id_list <- find_samples(monilinia, n = 200, cut = TRUE, progress = FALSE)
id_list
lengths(id_list)
print.table(monilinia[id_list[[1]], 1:10], zero.print = ".")
```

## Real-world example

Because you get a list of ids, it's good to see which ones are actually useful.
For this, you can calculate entropy. We will use the tidyverse to first create
a table of samples and data, calculate entropy for each row, and then join them
together. In general, we will want higher entropy values. For this, we will
load three tidyverse packages:

```{r loadtidy}
library("tibble")
library("tidyr")
library("dplyr")
```


```{r full_data, cache = TRUE}
# Generate and filter the possible data sets ------------------------------
set.seed(2018 - 03 - 07)
res <- find_samples(monilinia, n = 10000, cut = TRUE, progress = FALSE) %>%
  tibble::enframe(name = "index", value = "ids") %>%            # create a data frame of of list columns
  dplyr::mutate(n = lengths(ids)) %>%                           # count the number of indices in each row
  dplyr::rowwise() %>%                                          # set the data frame to be computed by row
  dplyr::mutate(dat = list(monilinia[ids, ,drop = FALSE])) %>%  # add the original data set to each row
  dplyr::mutate(nall = sum(colSums(dat, na.rm = TRUE) > 0))     # count the number of columns

# calculate entropy for each data set -------------------------------------
# The statistics returns a data frame, but because we've embedded the data,
# we must calculate this separately and then merge it later.
entro <- res %>% 
  dplyr::mutate(e = list(entropy(dat))) %>% # calculate stats for each row
  dplyr::select(index, e) %>%               # retain only stats and index
  tidyr::unnest()                           # spread out the columns

# sort by E5, and missingness. ----------------------------
res_sort <- res %>%
  dplyr::inner_join(entro, by = "index") %>%
  dplyr::arrange(-E5, missing)
res_sort
```

From here, we can see the samples

```{r viewsamples}
cat(res_sort$ids[[1]], sep = ", ")
```

And visualize the distribution:

```{r barplot, fig.width = 15, fig.height = 7}
res_sort$dat[[1]] %>%
  colSums(na.rm = TRUE) %>%
  sort() %>%
  barplot(las = 3)
```


### Calculating entropy by groups

We can group our variables as well.

```{r ebyg}
f <- gsub("[.][0-9]+", "", colnames(monilinia))
f <- factor(f, unique(f))
entromean <- res %>% 
  dplyr::mutate(e = list(entropy(dat, f = f))) %>% # calculate stats for each row
  dplyr::select(index, e) %>%               # retain only stats and index
  tidyr::unnest() %>%                       # spread out the columns
  dplyr::group_by(index) %>%
  dplyr::summarize_if(is.numeric, mean)     # calculate mean over all loci

# sort by E5, and missingness. ----------------------------
resmean_sort <- res %>%
  dplyr::inner_join(entromean, by = "index") %>%
  dplyr::arrange(-E5, missing)
resmean_sort
```

