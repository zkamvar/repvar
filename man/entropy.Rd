% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/entropy.R
\name{entropy}
\alias{entropy}
\title{Calculate Entropy Statistics}
\usage{
entropy(tab)
}
\arguments{
\item{tab}{a numeric matrix}
}
\value{
a data frame with three columns: \code{eH}, \code{G}, \code{E5}, and \code{missing}
}
\description{
Because it's possible to have multiple results with a minimum number of
samples, one way of assessing their importance is to calculate how
distributed the alleles are among the samples. This can be done with entropy
statistics
}
\details{
This function caluclates four statistics from your data using
variable counts.
\itemize{
\item eH: The exponentiation of shannon's entropy: \code{exp(sum(-x * log(x)))}
(Shannon, 1948)
\item G : Stoddart and Taylor's index, or inverse Simpson's
index: \code{1/sum(x * x)} (Stoddart and Taylor, 1988; Simpson, 1949)
\item E5: Evenness (5) the ratio between the above two estimates:
\code{(G - 1)/(eH - 1)} (Pielou, 1975)
\item missing: the percent missing data out of the total number of cells.
}

Both G and eH can be thought of as the number of equally abundant variables
to acheive the same observed diversity. Both G and eH give different weight
to variables based on their abundance, so we use evenness to describe how
uniform this distribution is.

Note that this version of Evenness is different than Shannon's Evenness,
which is \code{H/ln(S)} where S is the number of variables (in our case).
}
\examples{
data(monilinia)
entropy(monilinia)
set.seed(1999)
i <- find_samples(monilinia, n = 150, cut = TRUE)
entropy(monilinia[i[[1]], ])
entropy(monilinia[i[[2]], ])
}
\references{
Claude Elwood Shannon. A mathematical theory of communication. Bell Systems
Technical Journal, 27:379-423,623-656, 1948

Simpson, E. H. Measurement of diversity. Nature 163: 688, 1949
doi:10.1038/163688a0

J.A. Stoddart and J.F. Taylor. Genotypic diversity: estimation and prediction
in samples. Genetics, 118(4):705-11, 1988.

E.C. Pielou. Ecological Diversity. Wiley, 1975.
}
